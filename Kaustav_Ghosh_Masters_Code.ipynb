{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da7b66a6",
   "metadata": {},
   "source": [
    "# Prompt Optimization for Generative Art Models\n",
    "### Kaustav Ghosh\n",
    "### Department of Computer Science \n",
    "### University of Exeter \n",
    "### Exeter, UK\n",
    "### kg498@exeter.ac.uk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d185dd7a",
   "metadata": {},
   "source": [
    "## Data Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6b74fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f107ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the filepaths of all the JSOn files conmtaining our prompt data from Kaggle\n",
    "filepaths = []\n",
    "for dirname, _, filenames in os.walk('C:/Users/Kaustav Ghosh/OneDrive/Desktop/AI ART/Data'):\n",
    "    for filename in filenames:\n",
    "        filepaths.append(os.path.join(dirname, filename))\n",
    "print(f\"Found {len(filepaths)} files.\")\n",
    "\n",
    "# These are based of the discord iteractions users have with the Midjourney Bot\n",
    "COMPONENTS_FOR_INITIAL_AND_VARIATION = set(\n",
    "    ['U1', 'U2', 'U3', 'U4', '‚ü≥', 'V1', 'V2', 'V3', 'V4'])\n",
    "COMPONENTS_FOR_UPSCALE = set(\n",
    "    ['Make Variations', 'Upscale to Max', 'Light Upscale Redo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cfe242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_message_type(message):\n",
    "    \n",
    "    #Figures out the message type based on the UI components displayed.\n",
    "    for components in message[\"components\"]:\n",
    "        for component in components[\"components\"]:\n",
    "            if component[\"label\"] in COMPONENTS_FOR_INITIAL_AND_VARIATION:\n",
    "                # For (very few) messages that are supposedly initial or variation requests, the content indicates\n",
    "                # that they are actually upscale requests. We will just put these aside.\n",
    "                if \"Upscaled\" in message[\"content\"]:\n",
    "                    return \"INCONCLUSIVE\"\n",
    "                return \"INITIAL_OR_VARIATION\"\n",
    "            elif component[\"label\"] in COMPONENTS_FOR_UPSCALE:\n",
    "                return \"UPSCALE\"\n",
    "    return \"TEXT_MESSAGE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d93a114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(message):\n",
    "    \n",
    "    #Extracts the prompt from the message content, which is located between double stars.\n",
    "    content = message[\"content\"]\n",
    "    # Replace newlines with spaces; makes the regex below work.\n",
    "    content = content.replace(\"\\n\", \" \")\n",
    "    # Find the text enclosed by two consecutive stars.\n",
    "    BETWEEN_STARS = \"\\\\*\\\\*(.*?)\\\\*\\\\*\"\n",
    "    match = re.search(BETWEEN_STARS, content)\n",
    "    if match:\n",
    "        return match.group()[2:-2]  # Exclude the stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c3dd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(prompt):\n",
    "    \n",
    "    #Prompts can include both text and images; this method removes the prompt image URLs.\n",
    "    URL = \"<https[^<]*>?\\s\"\n",
    "    matches = re.findall(URL, prompt)\n",
    "    for match in matches:\n",
    "        prompt = prompt.replace(match, \"\")\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8136bfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "upscaled = []\n",
    "df=pd.DataFrame()\n",
    "for filepath in filepaths:\n",
    "    with open(filepath, \"r\") as f:\n",
    "        content = json.load(f)\n",
    "        for single_message_list in content[\"messages\"]:\n",
    "            assert len(single_message_list) == 1\n",
    "            message = single_message_list[0]\n",
    "            message_type = get_message_type(message)\n",
    "\n",
    "            if message_type not in [\"INITIAL_OR_VARIATION\", \"UPSCALE\"]:\n",
    "                continue  # Ignore direct text messages.\n",
    "\n",
    "            prompt = get_prompt(message)\n",
    "            if not prompt:\n",
    "                continue  # Discard malformed messages.\n",
    "                \n",
    "            # The goal of this dataset is to learn *text prompts*, so remove any image prompts.\n",
    "            text_prompt = remove_urls(prompt)\n",
    "            if message_type in [\"INITIAL_OR_VARIATION\"]:\n",
    "                upscaled.append(\"INITIAL_OR_VARIATION\")\n",
    "            else:\n",
    "                upscaled.append(\"UPSCALED\")\n",
    "            prompts.append(text_prompt)\n",
    "            \n",
    "print(f\"Extracted {len(prompts)} text prompts.\")\n",
    "\n",
    "df[\"Prompts\"]=prompts\n",
    "df[\"Type\"]= upscaled\n",
    "df.to_csv(\"LAKE.csv\")\n",
    "\n",
    "# Storing final output in Lake.csv\n",
    "df2 = pd.read_csv(\"C:/Users/Kaustav Ghosh/OneDrive/Desktop/AI ART/LAKE.csv\")\n",
    "rslt_df = df2[df2['Prompts'].isalpha() == True]   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433db9ae",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "Most of the Data cleaning has been done manually by me by going through the Lake.csv. This honestly was 70% of the total workload as there was lot of data with a lot of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac1d852",
   "metadata": {},
   "source": [
    "## Machine learning\n",
    "we have used the cleaned Lake.csv to run in 3 of our models LSTM, RNN and GRU to accurately predicted upscaled chances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb3653b",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cdfb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "# Load the dataset from CSV file\n",
    "dataset = pd.read_csv(\"C:/Users/Kaustav Ghosh/OneDrive/Desktop/AI ART/LAKE.csv\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = dataset.iloc[:, 0].values  # Prompts\n",
    "y = dataset.iloc[:, 1].values  # Updated or not labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the prompts\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "X_train_tokens = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_tokens = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences for consistent input length\n",
    "max_sequence_length = max(len(tokens) for tokens in X_train_tokens)\n",
    "X_train_padded = pad_sequences(X_train_tokens, maxlen=max_sequence_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_tokens, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "y_train_encoded = label_encoder.transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_sequence_length))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train_padded, y_train_encoded, validation_data=(X_test_padded, y_test_encoded), epochs=10, batch_size=32)\n",
    "\n",
    "# Function to generate an optimized prompt\n",
    "def generate_optimized_prompt(prompt):\n",
    "    prompt_tokens = tokenizer.texts_to_sequences([prompt])\n",
    "    prompt_tokens_padded = pad_sequences(prompt_tokens, maxlen=max_sequence_length, padding='post')\n",
    "    prediction = model.predict(prompt_tokens_padded)\n",
    "    prediction_label = label_encoder.inverse_transform([1 if p > 0.5 else 0 for p in prediction])[0]\n",
    "    return prompt if prediction_label == \"Already Optimized\" else prompt + \"Need Optimization\"\n",
    "\n",
    "# Usage\n",
    "input_prompt = input(\"User prompt\")\n",
    "optimized_prompt = generate_optimized_prompt(input_prompt)\n",
    "print(\"Optimized prompt:\", optimized_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576b2661",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bf44c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load the dataset from CSV file\n",
    "dataset = pd.read_csv(\"C:/Users/Kaustav Ghosh/OneDrive/Desktop/AI ART/LAKE.csv\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = dataset.iloc[:, 0].values  # Prompts\n",
    "y = dataset.iloc[:, 1].values  # Updated or not labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the prompts\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "X_train_tokens = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_tokens = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences for consistent input length\n",
    "max_sequence_length = max(len(tokens) for tokens in X_train_tokens)\n",
    "X_train_padded = pad_sequences(X_train_tokens, maxlen=max_sequence_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_tokens, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "y_train_encoded = label_encoder.transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Define the RNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_sequence_length))\n",
    "model.add(SimpleRNN(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train_padded, y_train_encoded, validation_data=(X_test_padded, y_test_encoded), epochs=10, batch_size=32)\n",
    "\n",
    "# Function to generate an optimized prompt\n",
    "def generate_optimized_prompt(prompt):\n",
    "    prompt_tokens = tokenizer.texts_to_sequences([prompt])\n",
    "    prompt_tokens_padded = pad_sequences(prompt_tokens, maxlen=max_sequence_length, padding='post')\n",
    "    prediction = model.predict(prompt_tokens_padded)\n",
    "    prediction_label = label_encoder.inverse_transform([1 if p > 0.5 else 0 for p in prediction])[0]\n",
    "    return prompt if prediction_label == \"Already Optimized\" else prompt + \"Need Optimization\"\n",
    "\n",
    "# Usage\n",
    "input_prompt = input(\"User prompt\")\n",
    "optimized_prompt = generate_optimized_prompt(input_prompt)\n",
    "print(\"Optimized prompt:\", optimized_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bc060d",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dada99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load the dataset from CSV file\n",
    "dataset = pd.read_csv(\"C:/Users/Kaustav Ghosh/OneDrive/Desktop/AI ART/LAKE.csv\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = dataset.iloc[:, 0].values  # Prompts\n",
    "y = dataset.iloc[:, 1].values  # Updated or not labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the prompts\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "X_train_tokens = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_tokens = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences for consistent input length\n",
    "max_sequence_length = max(len(tokens) for tokens in X_train_tokens)\n",
    "X_train_padded = pad_sequences(X_train_tokens, maxlen=max_sequence_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_tokens, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "y_train_encoded = label_encoder.transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Define the GRU model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_sequence_length))\n",
    "model.add(GRU(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train_padded, y_train_encoded, validation_data=(X_test_padded, y_test_encoded), epochs=10, batch_size=32)\n",
    "\n",
    "# Function to generate an optimized prompt\n",
    "def generate_optimized_prompt(prompt):\n",
    "    prompt_tokens = tokenizer.texts_to_sequences([prompt])\n",
    "    prompt_tokens_padded = pad_sequences(prompt_tokens, maxlen=max_sequence_length, padding='post')\n",
    "    prediction = model.predict(prompt_tokens_padded)\n",
    "    prediction_label = label_encoder.inverse_transform([1 if p > 0.5 else 0 for p in prediction])[0]\n",
    "    return prompt if prediction_label == \"Already Optimized\" else prompt + \"Need Optimization\"\n",
    "\n",
    "# Usage\n",
    "input_prompt = input(\"User prompt\")\n",
    "optimized_prompt = generate_optimized_prompt(input_prompt)\n",
    "print(\"Optimized prompt:\", optimized_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
